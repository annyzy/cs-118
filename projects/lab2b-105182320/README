NAME: Ziying Yu
EMAIL: annyu@g.ucla.edu
ID: 105182320

Files:
1)SortedList.h:
	a header file containing interfaces for linked list operations.
2)SortedList.c:
	the source for a C source module that compiles cleanly and implements insert, delete, lookup, and length methods for a sorted doubly linked list.
3)lab2_list.c:
	the source for a C program that compiles and implements the specified command line options (--threads, --iterations, --yield, --sync, --lists), drives one or more parallel threads that do operations on a shared linked list, and reports on the final list and performance. Note that we expect segmentation faults in non-synchronized multi-thread runs, so your program should catch those and report the run as having failed.
4)Makefile:
	build the deliverable programs, output, graphs, and tarball.
	default ... the lab2_list executable (compiling with the -Wall and -Wextra options).
	tests ... run all specified test cases to generate CSV results
	profile ... run tests with profiling tools to generate an execution profiling report
	graphs ... use gnuplot to generate the required graphs
	dist ... create the deliverable tarball
	clean ... delete all programs and output generated by the Makefile
5)lab2b_list.csv:
	containing the results for all of test runs.
6)profile.out:
	execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.
7)graphs
	lab2b_1.png:throughput vs. number of threads for mutex and spin-lock synchronized list operations.
	lab2b_2.png:mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
	lab2b_3.png:successful iterations vs. threads for each synchronization method.
	lab2b_4.png:throughput vs. number of threads for mutex synchronized partitioned lists.
	lab2b_5.png:throughput vs. number of threads for spin-lock-synchronized partitioned lists.
8)README
QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

The list operation spent most of the CPU time in the 1 and 2-thread list tests in the function called in pthread_create. Since there are only few threads, locking and spinning(context switches) may spent less CPU cycles to do the synchronization compare to the list operations.

In the high-thread spin-locks tests, if there are lots of threads in the lists, spin-lock may cost the most of the CPU time. Since there is only one thread can hold the lock and do the work in the critical sections, all the others threads have to keep spinning to wait for the lock to be available.

In the high-mutex tests, most the CPU time in spent in the list operations if the list is long. If the list length is not very long, then most the CPU time is spent in context-switch. When the other threads find the lick has been held, it uses system calls to sleep which are expensive.

QUESTION 2.3.2 - Execution Profiling:
From the output of the profile.out:
Total: 840 samples
     780  92.9%  92.9%      840 100.0% thread_worker
      53   6.3%  99.2%       53   6.3% __strcmp_sse42
       7   0.8% 100.0%       54   6.4% SortedList_insert
       0   0.0% 100.0%        6   0.7% SortedList_lookup
       0   0.0% 100.0%      840 100.0% __clone
       0   0.0% 100.0%      840 100.0% start_thread

339    339  364: 			while (__sync_lock_test_and_set(&lock[list[i]], 1));
441    441  505: 			while (__sync_lock_test_and_set(&lock[list[i]], 1)) while (lock[list[i]]);
...
As we can see, while (__sync_lock_test_and_set(&lock[list[i]], 1)) consume most of the CPU time when the spin-lock version the list exerciser is run with a large number of threads. The reason is that only one thread can obtain the lock and do the work in the critical section, all the other locks have to run this line of the lock several time to wait for the lock to be available. If there is a large number of the threads, there will be more contention so the 

QUESTION 2.3.3 - Mutex Wait Time:
The reason that the average lock-wait time rise so dramatically with the number of contending threads is that there are more threads competing with each other to obtain the single lock and running the mutex checking operations to see if the lock is available.

The reason that the completion time per operation rise less dramatically with the number of contending threads is that every other threads have to wait for the resource to be available while other threads have locked it, thus increase the completion time. But a single thread completion time per operation is independent of the total number of threads, so it is a less dramatic rise.

The reason that the wait time per operation to go up faster than the completion time per operation is that each single thread has their own wait time but there are many overlap wait time for all the threads who are waiting. The completion time do not overlap.

QUESTION 2.3.4 - Performance of Partitioned Lists
For both the mutex synchronization and spin-lock-synchronized, as the number of the lists increase, the performance would get better because the sublist of the list is divided and they could run concurrently.

It is not necessary that the throughput continue increasing as the number of lists is further increased. For example, it may depends on the CPU power in which how many threads is their limits to run concurrently and the possibility of contention could be 0 if it exceed the limit.
In addition, the throughput may decrease as the increased number of lock operations who run with increasing parallelism.

It is not the case because partition shorten the list but each partitioned sublist spend more time on lock and unlock, which result inequivalent.

